{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcript A from Lecture, October 26, 2021\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "########################################\n",
    "# Change the string in the line below! #\n",
    "########################################\n",
    "sys.path.append(\"/Users/gilbert/Documents/CS111-2021-fall/Python\") \n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import numpy.linalg as npla\n",
    "import scipy\n",
    "from scipy import linalg as spla\n",
    "import scipy.sparse\n",
    "import scipy.sparse.linalg\n",
    "from scipy import integrate\n",
    "import networkx as nx\n",
    "import cs111\n",
    "\n",
    "##########################################################\n",
    "# If this import for matplotlib doesn't work, try saying #\n",
    "#   conda install -c conda-forge ipympl                  #\n",
    "# at a shell prompt on your computer                     #\n",
    "##########################################################\n",
    "import matplotlib\n",
    "%matplotlib ipympl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "np.set_printoptions(precision = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD theorems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.rand(8,5)\n",
    "print('shape of A:', A.shape)\n",
    "print()\n",
    "\n",
    "U,sigma,Vt = spla.svd(A)\n",
    "\n",
    "print('shape of U:', U.shape)\n",
    "print('sigma:', sigma)\n",
    "print('shape of Vt:', Vt.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Theorem 1. The rank of $A$ is the number of nonzero singular values.</h3>\n",
    "\n",
    "In our example, sure enough, the matrix $A$ has rank 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('singular values:', sigma)\n",
    "print('rank(A):', npla.matrix_rank(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In floating-point arithmetic, what counts as \"nonzero\" can be a judgement call. To illustrate this, we form a new 8-by-5 matrix $B$ whose first 4 columns are the same as $A$ and whose 5th column is the sum of the first 4 columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = A.copy()\n",
    "B[:,4] = A @ [1, 1, 1, 1, 0]\n",
    "# can you see why this replaces column 4 of B with the sum of the first 4 columns of A?\n",
    "\n",
    "print('B:\\n', B)\n",
    "print()\n",
    "print('B @ [1, 1, 1, 1, -1] should be the zero vector:\\n\\n', B @ [1, 1, 1, 1, -1])\n",
    "print()\n",
    "print('norm of B @ [1, 1, 1, 1, -1]:', npla.norm(B @ [1, 1, 1, 1, -1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically speaking, $B$ should have rank 4 because one column is linearly dependent on the other four columns. In floating-point arithmetic, though, round-off error makes the last column not quite exactly equal to the sum of the others. The exact rank of this perturbed matrix is 5, and sure enough all of its computed singular values are nonzero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UB, sigmaB, VtB = spla.svd(B)\n",
    "print('singular values of B:', sigmaB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, because only 4 of the singular values are significantly larger than machine epsilon, we (and numpy) say that $B$ has \"numerical rank\" 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npla.matrix_rank(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined the 2-norm of a matrix a while ago, \n",
    "but we never talked about algorithms to compute it.\n",
    "It turns out that the SVD gives us an algorithm.\n",
    "\n",
    "<h3>Theorem 2. The 2-norm $||A||_2$ is equal to the largest singular value $\\sigma_0$.</h3>\n",
    "\n",
    "This is because the norm of $A$ is the largest stretch it applies to any vector, \n",
    "\n",
    "$$||A||_2 = \\max_x (||Ax||_2/||x||_2).$$\n",
    "\n",
    "We saw that $A$ maps the unit sphere to an ellipsoid, and the largest stretch happens at the longest axis of the ellipsoid, whose length is $\\sigma_0$. The actual vectors are $x=v_0$ and $Ax = \\sigma_0 u_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v0 = V[:,0]\n",
    "u0 = U[:,0]\n",
    "\n",
    "print('singular values of A:', sigma)\n",
    "print('2-norm of A:', npla.norm(A,2))\n",
    "print()\n",
    "\n",
    "print('v_0:', v0)\n",
    "print('u_0:', u0)\n",
    "print('A @ v_0:', A@v0)\n",
    "print()\n",
    "print('norm(A @ v_0) / norm(v_0) :', npla.norm(A@v0, 2) / npla.norm(v0, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVD also gives us a way to compute the condition number of $A$ in the 2-norm, $\\kappa_2(A)$.\n",
    "Recall the definition\n",
    "\n",
    "$$\\kappa(A) = \\max(\\mbox{stretch}) / \\min(\\mbox{stretch}),$$\n",
    "\n",
    "the ratio of the maximum amount any vector is stretched by $A$ to the minimum amount \n",
    "any vector is stretched by $A$. \n",
    "(The minimum is 0 if $A$ is square and singular, \n",
    "or more generally if the rank of $A$ is less than the number of columns,\n",
    "and in that case the condition number is infinite.\n",
    "If $A$ is square and nonsingular, the condition number is $||A|| / ||A^{-1}||.$)\n",
    "\n",
    "From the discussion of $||A||_2$ above, it is clear that\n",
    "\n",
    "<h3>Theorem 3. The 2-norm condition number  $\\kappa_2(A)$ is equal to the ratio of the largest and smallest singular values,</h3>\n",
    "\n",
    "$$\\kappa_2(A) = \\sigma_0 / \\sigma_{\\min(m,n)-1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ratio of extreme singular values: ', sigma[0]/sigma[-1])\n",
    "#print('2-norm condition number of matrix:', npla.cond(A,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfortunately numpy's condition number routine doesn't work for non-square matrices!\n",
    "\n",
    "npla.cond(A,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _Frobenius norm_ of a matrix is the square root of the sum of the squares of all\n",
    "of its elements. It's as if we took the whole $m$-by-$n$ matrix as a vector in $mn$-dimensional space and computed its Euclidean length. This is an easy norm to compute, so we often use it to compare two matrices with each other. It turns out the SVD gives the Frobenius norm too.\n",
    "\n",
    "<h3>Theorem 4. The Frobenius norm $||A||_F$ is equal to $(\\sum_i \\sigma_i^2)^{1/2}$.</h3>\n",
    "\n",
    "Let's check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumsig = 0\n",
    "for i in range(len(sigma)):\n",
    "    sumsig += sigma[i]**2\n",
    "print('sqrt(sum(singular values squared)):', np.sqrt(sumsig))\n",
    "\n",
    "nrows, ncols = A.shape\n",
    "sumsqA = 0\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        sumsqA += A[i,j]**2\n",
    "print('sqrt(sum(matrix elements squared)):', np.sqrt(sumsqA))\n",
    "\n",
    "print('Frobenius norm of matrix:          ', npla.norm(A,'fro'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The determinant of a matrix is important in matrix theory, \n",
    "but is hardly ever computed in numerical linear algebra where matrix norms and\n",
    "condition numbers are more useful.\n",
    "However, the SVD does give a way to compute the determinant of a (square) matrix,\n",
    "up to its sign.\n",
    "\n",
    "<h3>Theorem 5. The determinant of a square matrix is $\\pm$ the product of its singular values, $\\prod_i \\sigma_i$.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asquare = A[:5,:]\n",
    "print('shape of square matrix:', Asquare.shape)\n",
    "UAs, sigmaAs, VtAs = npla.svd(Asquare)\n",
    "\n",
    "prodsig = 1\n",
    "for i in range(len(sigmaAs)):\n",
    "    prodsig *= sigmaAs[i]\n",
    "print('product of singular values:', prodsig)\n",
    "\n",
    "print('determinant of matrix:     ', npla.det(Asquare))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sum of rank-1 matrices, and low-rank approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final theorem is just a simple way to rewrite the SVD equation $A=USV^T$.\n",
    "Recall that $u_i$ is column $i$ of $U$ and $v_i$ is column $i$ of $V$, so $v_i^T$ is row $i$ of $V^T$.\n",
    "It is straightforward to check algebraically that\n",
    "\n",
    "<h3>Theorem 6. Matrix $A$ is the sum of rank-1 matrices: $A = \\sum_{i=0}^{\\min(m,n)}\\sigma_i u_i v_i^T$ </h3>\n",
    "\n",
    "Each term in the sum is a scalar multiple of the outer product $u_i v_i^T$,\n",
    "which is an $m$-by-$n$ matrix whose rank is 1;\n",
    "it's essentially the multiplication table of the elements of $u_i$ (as rows) times\n",
    "the elements of $v_i$ (as columns).\n",
    "(Indeed, every matrix product $AB$ can be written as a sum of rank-1 matrices, \n",
    "each of which is the outer product of a column of $A$ and a row of $B$.)\n",
    "\n",
    "Though it's just a humble algebraic identity, this last theorem actually motivates\n",
    "the greatest applications of SVD in data analysis. \n",
    "We'll see that in the next section, but first let's just check the theorem numerically \n",
    "for our example matrix.\n",
    "\n",
    "We ought to be able to compute $u_i v_i^T$ as U[:,i] @ V[:,i].T in numpy.\n",
    "Unfortunately numpy is broken here -- it gets confused because it can't tell that U[:,i] is a column vector and V[:,i].T is a row vector, and it does the wrong thing.\n",
    "(Matlab on the other hand gets this right.)\n",
    "In numpy, we have to use the np.outer() function to compute the outer product of two vectors,\n",
    "as we do below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asum = np.zeros(A.shape)\n",
    "for i in range(len(sigma)):\n",
    "    Asum += sigma[i] * np.outer(U[:,i], V[:,i])\n",
    "\n",
    "print('norm of difference between Asum and A:',  npla.norm(Asum - A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we truncate the sum in Theorem 6 (above) after some number $k<\\min(m,n)$ of terms? Let us define\n",
    "\n",
    "$$A_k = \\sum_{i=0}^{k-1}\\sigma_i u_i v_i^T,$$\n",
    "\n",
    "with $A_{\\min(m,n)} = A$.\n",
    "We can think of each rank-1 term $\\sigma_i u_i v_i^T$ as adding some \"weight\" to the matrix, in an informal sense. The terms are added in order of decreasing weight, because $\\sigma_0 \\ge \\sigma_1 \\ge \\cdots$. If the first singular values are much larger than the later ones, we might hope that $A_k$ would be a good approximation to $A$ for small values of $k$. This turns out to be true in a very strong sense: $A_k$ is the _best_ possible rank-$k$ approximation to $A$, as measured in either the 2-norm or the Frobenius norm.\n",
    "\n",
    "<h3> Theorem 7. Among all $m$-by-$n$ matrices $B_k$ that have rank $k$, \n",
    "the minimum possible value of $||A-B_k||_2$ is attained when $B_k=A_k$ as defined above.\n",
    "That value is $||A-A_k||_2 = \\sigma_{k}$.</h3>\n",
    "\n",
    "<h3> Theorem 8. Among all $m$-by-$n$ matrices $B_k$ that have rank $k$, \n",
    "the minimum possible value of $||A-B_k||_F$ is attained when $B_k=A_k$.\n",
    "That value is $||A-A_k||_F = (\\sum_{i\\ge k}\\sigma_{k}^2)^{1/2}$. </h3>\n",
    "\n",
    "For the case $k=0$, we get the results of Theorems 2 and 4 above. \n",
    "For $k\\ge \\mbox{rank}(A)$, we get $A=A_k$.\n",
    "\n",
    "We illustrate Theorem 7 with our sample 8-by-5 matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows, ncols = A.shape\n",
    "print('shape of A:', (nrows, ncols))\n",
    "\n",
    "U,sigma,Vt = spla.svd(A)\n",
    "\n",
    "print('singular values:', sigma)\n",
    "print('rank(A):', npla.matrix_rank(A))\n",
    "print()\n",
    "\n",
    "Ak = np.zeros(A.shape)\n",
    "for k in range(len(sigma)):\n",
    "    print('rank', npla.matrix_rank(Ak), 'approximation: 2-norm(A%d - A) =' % k, npla.norm(Ak-A,2))\n",
    "    Ak += sigma[k] * np.outer(U[:,k], Vt[k,:])\n",
    "print('rank', npla.matrix_rank(Ak), 'approximation: 2-norm(A%d - A) =' % len(sigma), npla.norm(Ak-A,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
