{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent and its relatives\n",
    "\n",
    "The first window below does the standard imports for CS 111. The list may change as the quarter goes on.\n",
    "\n",
    "You should change the string in **sys.path.append()** to be the full path on your own computer to the **CS111-2021-fall/Python** directory that you got when you cloned **CS111-2021-fall** to your computer from GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "########################################\n",
    "# Change the string in the line below! #\n",
    "########################################\n",
    "sys.path.append(\"/Users/gilbert/Documents/CS111-2021-fall/Python\") \n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.linalg as npla\n",
    "from autograd import elementwise_grad, value_and_grad, grad\n",
    "from scipy.optimize import minimize\n",
    "from collections import defaultdict\n",
    "from itertools import zip_longest\n",
    "from functools import partial\n",
    "import scipy\n",
    "from scipy import linalg as spla\n",
    "import scipy.sparse\n",
    "import scipy.sparse.linalg\n",
    "from scipy import integrate\n",
    "import networkx as nx\n",
    "import json\n",
    "\n",
    "import cs111\n",
    "\n",
    "#######################################################\n",
    "# Here are three different ways to have plots appear. #\n",
    "# Uncomment the one you want to use.                  #\n",
    "#                                                     #\n",
    "# inline    : static plot in notebook                 #\n",
    "# ipympl    : plot in notebook with pan/zoom controls #\n",
    "# tk        : plot in popup window with pan/zoom      #\n",
    "#                                                     #\n",
    "# If %matplotlib ipympl doesn't work, try saying:     #\n",
    "#   conda install -c conda-forge ipympl               #\n",
    "# at a shell prompt.                                  #\n",
    "#######################################################\n",
    "import matplotlib\n",
    "# %matplotlib inline \n",
    "# %matplotlib ipympl\n",
    "%matplotlib tk \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib import animation\n",
    "\n",
    "np.set_printoptions(precision = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function, the grid we will visualize it on, and the starting point.\n",
    "# Also, since we know the minimum of each of these functions, record that for reference too.\n",
    "\n",
    "function = 'Strang'\n",
    "\n",
    "if function == 'Beale':\n",
    "    # Beale's function. See https://en.wikipedia.org/wiki/Test_functions_for_optimization\n",
    "    f  = lambda x, y: (1.5 - x + x*y)**2 + (2.25 - x + x*y**2)**2 + (2.625 - x + x*y**3)**2\n",
    "    xmin, xmax, xstep = -4.5, 4.5, .2\n",
    "    ymin, ymax, ystep = -4.5, 4.5, .2\n",
    "    contours = np.logspace(0, 5, 35)\n",
    "    minima = np.array([3., .5])\n",
    "    x0 = np.array([3., 4.])\n",
    "    \n",
    "elif function == 'Rosenbrock':\n",
    "    # Rosenbrock's function. See https://en.wikipedia.org/wiki/Test_functions_for_optimization\n",
    "    alpha = 100\n",
    "    f = lambda x, y: (1.0 - x)**2 + alpha * (y-x**2)**2\n",
    "    xmin, xmax, xstep = -3.5, 2., .05\n",
    "    ymin, ymax, ystep = -5., 5., .05\n",
    "    contours = np.logspace(0, 5, 35)\n",
    "    minima = np.array([1., 1.]) \n",
    "    x0 = np.array([-3., -4.])\n",
    "    \n",
    "elif function == 'Strang':\n",
    "    # The simple quadratic from Gil Strang's lectures on optimization\n",
    "    alpha = 0.1\n",
    "    f = lambda x, y: (x**2 + alpha*y**2) / 2 \n",
    "    xmin, xmax, xstep = -1., 1., .1\n",
    "    ymin, ymax, ystep = -1., 1.5, .1\n",
    "    contours = np.logspace(-2.5, 0, 15)\n",
    "    minima = np.array([0.,0.])\n",
    "    x0 = np.array([alpha,1.])\n",
    "    \n",
    "else:\n",
    "    assert False\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a 2D mesh of points with a function value for each one, for visualization\n",
    "\n",
    "x, y = np.meshgrid(np.arange(xmin, xmax + xstep, xstep), np.arange(ymin, ymax + ystep, ystep))\n",
    "z = f(x, y)\n",
    "\n",
    "# Get a version of the function that knows its gradient. (Sounds like magic, huh?)\n",
    "# This routine uses automatic differentiation (= backpropagation) and yields a new callable \n",
    "# \"func\" that takes as argument the numpy array np.array([x,y]) \n",
    "# and returns the pair (f(x,y), gradient vector at (x,y))\n",
    "\n",
    "func = value_and_grad(lambda args: f(*args))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a 3D surface plot of the objective function \n",
    "\n",
    "fig = plt.figure(figsize=(8, 5))\n",
    "ax = plt.axes(projection='3d', elev=50, azim=-50)\n",
    "\n",
    "ax.plot_surface(x, y, z, norm=LogNorm(), rstride=1, cstride=1, \n",
    "                edgecolor='none', alpha=.8, cmap=plt.cm.jet)\n",
    "ax.plot(*minima, f(*minima), 'r*', markersize=10)\n",
    "\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "ax.set_zlabel('$z$')\n",
    "\n",
    "ax.set_xlim((xmin, xmax))\n",
    "ax.set_ylim((ymin, ymax))\n",
    "ax.set_title(f\"3D surface plot of {function} function\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good parameters to show zigzag behavior with Strang's function\n",
    "\n",
    "rate = 1.8\n",
    "res = cs111.gradient_descent(func, x0, \n",
    "            rate=rate, max_iters=100, tol=.0001)\n",
    "print(f\"{function}: rate = {rate}, {res.nit} iterations, {res.message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A good learning rate for Rosenbrock is delicate (bad version)\n",
    "\n",
    "rate = .001\n",
    "res = cs111.gradient_descent(func, x0, \n",
    "            rate=rate, max_iters=10000, tol=.01)\n",
    "print(f\"{function}: rate = {rate}, {res.nit} iterations, {res.message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.costs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A good learning rate for Rosenbrock is delicate (good version)\n",
    "\n",
    "rate = .0002\n",
    "res = cs111.gradient_descent(func, x0, \n",
    "            rate=rate, max_iters=10000, tol=.0001) # try tol=.0001 and .01\n",
    "print(f\"{function}: rate = {rate}, {res.nit} iterations, {res.message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the objective function at each iteration\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(res.costs, '.-')\n",
    "ax.set_xlabel('iteration')\n",
    "ax.set_ylabel('objective function value')\n",
    "ax.set_title(f\"{function}: {res.message}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contour plot of objective function showing path of optimization\n",
    "\n",
    "path = np.array(res.xks).T\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.contour(x, y, z, levels=contours, norm=LogNorm(), cmap=plt.cm.jet)\n",
    "\n",
    "ax.quiver(path[0,:-1], path[1,:-1], path[0,1:]-path[0,:-1], path[1,1:]-path[1,:-1], \n",
    "          scale_units='xy', angles='xy', scale=1, width=(xmax-xmin)/1000, color='k')\n",
    "ax.plot(*minima, 'r*', markersize=18)\n",
    "\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "\n",
    "ax.set_xlim((xmin, xmax))\n",
    "ax.set_ylim((ymin, ymax))\n",
    "ax.axis('equal')\n",
    "ax.set_title(f\"Gradient descent for {function} function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good parameters to show effect of momentum with Strang's function\n",
    "\n",
    "rate = 1.8\n",
    "beta = .45\n",
    "res2 = cs111.gradient_momentum(func, x0, \n",
    "            rate=rate, beta=beta, max_iters=100, tol=.0001)\n",
    "print(f\"{function}: rate = {rate}, beta = {beta}, {res2.nit} iterations, {res.message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying momentum parameters for Rosenbrock\n",
    "\n",
    "rate = .0002\n",
    "beta = .75  # try both .6 and .75\n",
    "res2 = cs111.gradient_momentum(func, x0, \n",
    "        rate=rate, beta=beta, max_iters=10000, tol=.0001) #  try .01 and .0001\n",
    "print(f\"{function}: rate = {rate}, beta = {beta}, {res2.nit} iterations, {res2.message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contour plot of objective function showing both gradient descent and momentum methods\n",
    "\n",
    "path1 = np.array(res.xks).T\n",
    "path2 = np.array(res2.xks).T\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.contour(x, y, z, levels=contours, norm=LogNorm(), cmap=plt.cm.jet)\n",
    "\n",
    "ax.quiver(path1[0,:-1], path1[1,:-1], path1[0,1:]-path1[0,:-1], path1[1,1:]-path1[1,:-1], \n",
    "          scale_units='xy', angles='xy', scale=1, width=(xmax-xmin)/1000, color='k', label='gradient')\n",
    "\n",
    "ax.quiver(path2[0,:-1], path2[1,:-1], path2[0,1:]-path2[0,:-1], path2[1,1:]-path2[1,:-1], \n",
    "          scale_units='xy', angles='xy', scale=1, width=(xmax-xmin)/1000, color='r', label='momentum')\n",
    "ax.plot(*minima, 'r*', markersize=18)\n",
    "\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "\n",
    "ax.set_xlim((xmin, xmax))\n",
    "ax.set_ylim((ymin, ymax))\n",
    "ax.axis('equal')\n",
    "ax.set_title(f\"Comparison for {function} function\")\n",
    "ax.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the objective function at each iteration of both methods\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(res.costs, '.-', label='gradient descent')\n",
    "ax.plot(res2.costs, '.-', label='with momentum')\n",
    "ax.set_xlabel('iteration')\n",
    "ax.set_ylabel('objective function value')\n",
    "ax.set_title(f\"Comparison of methods for {function}\")\n",
    "ax.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
